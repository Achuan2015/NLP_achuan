{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证 Whoosh的工呢过"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入必要的依赖包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(\"../\")\n",
    "from whoosh.index import create_in, open_dir\n",
    "from whoosh.fields import *\n",
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "from jieba.analyse.analyzer import ChineseAnalyzer\n",
    "\n",
    "analyzer = ChineseAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义 SSC 所需要的 Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_schema = Schema(id=ID(stored=True), corpus_id=ID(stored=True), field=ID(stored=True), content=TEXT(stored=True, analyzer=analyzer))\n",
    "\n",
    "\n",
    "if not os.path.exists(\"corpus_index_dir\"):\n",
    "    os.mkdir(\"corpus_index_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 添加文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Dump cache file failed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/opts/anaconda3/lib/python3.8/site-packages/jieba/__init__.py\", line 154, in initialize\n",
      "    _replace_file(fpath, cache_file)\n",
      "PermissionError: [Errno 1] Operation not permitted: '/tmp/tmphghd02cu' -> '/tmp/jieba.cache'\n",
      "Loading model cost 1.286 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "ix = create_in('corpus_index_dir', corpus_schema, 'bot_1')\n",
    "writer = ix.writer()\n",
    "writer.add_document(id='123', corpus_id='1', field='faq', content='你们公司在哪里')\n",
    "writer.add_document(id='124', corpus_id='1', field='faq', content='你们公司在几号线')\n",
    "writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_schema2 = Schema(id=NUMERIC(int, 64, stored=True), corpus_id=NUMERIC(stored=True), field=ID(stored=True), content=TEXT(stored=True, analyzer=analyzer))\n",
    "ix2 = create_in('corpus_index_dir', corpus_schema, 'bot_2')\n",
    "writer2 = ix2.writer()\n",
    "writer2.add_document(id='123', corpus_id='1', field='faq', content='你们公司在哪里')\n",
    "writer2.add_document(id='124', corpus_id='1', field='faq', content='你们公司在几号线')\n",
    "writer2.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提交变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 重新加载文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whoosh.index as index\n",
    "\n",
    "ix = index.open_dir('corpus_index_dir', indexname='bot_1')\n",
    "parser = QueryParser('content', schema=ix.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '你们公司在哪里', 'corpus_id': '1', 'field': 'faq', 'id': '123'}\n"
     ]
    }
   ],
   "source": [
    "with ix.searcher() as searcher:\n",
    "    writer = ix.writer()\n",
    "    for fields in searcher.all_stored_fields():\n",
    "        print(fields)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with ix.searcher() as s:\n",
    "    q = parser.parse('公司')\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        # print(hit.highlights(\"content\")\n",
    "        print(hit.get('field'))\n",
    "    print(\"==\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试删除 document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer1 = ix.writer()\n",
    "# writer1.delete_by_term('id', '123')\n",
    "writer1.update_document(id='124', corpus_id='1', field='faq', content='你们公司在几号线!!')\n",
    "\n",
    "writer1.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faq 你们公司在几号线\n",
      "faq 你们公司在几号线!!\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "ix = index.open_dir('corpus_index_dir', indexname='bot_1')\n",
    "parser = QueryParser('content', schema=ix.schema)\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    q = parser.parse('公司')\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        # print(hit.highlights(\"content\")\n",
    "        print(hit.get('field'), hit.get('content'))\n",
    "    print(\"==\" * 10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dafaadbda7e2b4673c53a13d22366aab1c2a35a28f6b7c4e8046d1a490edd4db"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
